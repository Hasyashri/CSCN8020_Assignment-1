{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0643da03",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f28df2",
   "metadata": {},
   "source": [
    "## **Problem 1 - Pick-and-Place Robot as an MDP**\n",
    "\n",
    "> **What is the goal?**\n",
    "\n",
    "We want a robot arm to pick up an object and place it somewhere. It should do this in a way that is fast and smooth.\n",
    "\n",
    "To make this happen, we use Reinforcement Learning (RL) — and first, we turn this into a Markov Decision Process (MDP).\n",
    "\n",
    "> **MDP Components (What RL needs to learn):**\n",
    "\n",
    "- States (S) – What the robot knows about the world right now.\n",
    "- Actions (A) – What the robot can do next.\n",
    "- Rewards (R) – How good or bad the action was.\n",
    "- Transitions (T) – What happens when you do an action in a state.\n",
    "\n",
    "**Goal – Learn a policy (way to act) to maximize rewards over time.**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Define States (S)**\n",
    "\n",
    "**The state is the situation the robot is in.**\n",
    "\n",
    "State can include:\n",
    "\n",
    "- Position of robot arm joints (angles)\n",
    "- Velocity of each joint\n",
    "- Position of the object to be picked\n",
    "- Position of where to place it\n",
    "\n",
    "### Example:\n",
    "\n",
    "S = [joint1_angle, joint1_velocity, joint2_angle, joint2_velocity, object_position, target_position]\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd849f9",
   "metadata": {},
   "source": [
    "#### **Step 2: Define Actions (A)**\n",
    "\n",
    "**The actions are what the robot can do.**\n",
    "\n",
    "Actions could be:\n",
    "\n",
    "- Increase or decrease motor speed\n",
    "- Move joint slightly left or right\n",
    "- Open or close the gripper\n",
    "\n",
    "**Example:**\n",
    "\n",
    "`A = [move_joint1_left, move_joint1_right, move_joint2_left, move_joint2_right, open_gripper, close_gripper]`\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Define Rewards (R)\n",
    "\n",
    "**Rewards tell the robot if it did good or bad.**\n",
    "\n",
    "- +10 → If the object is successfully placed at the target\n",
    "- +5 → If the object is picked up\n",
    "- -1 → If the movement is jerky or slow\n",
    "- -5 → If the robot drops the object or misses\n",
    "\n",
    "---\n",
    "\n",
    "####  **Step 4: Transitions (T)**\n",
    "\n",
    "What happens when an action is taken in a state.\n",
    "\n",
    "In this robot, transitions are deterministic (we can predict what happens)\n",
    "\n",
    "**For example:**\n",
    "\n",
    "`If joint1 moves right by 5°, then next angle = angle + 5°`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f49607",
   "metadata": {},
   "source": [
    "#### ** Final MDP Summary:**\n",
    "\n",
    "**States (S):**\n",
    "\n",
    "    Positions and velocities of robot arm joints, object location, target location\n",
    "\n",
    "**Actions (A):**\n",
    "    Small motor commands: move joints, open/close gripper\n",
    "\n",
    "**Rewards (R):**\n",
    "    +10 for placing object correctly\n",
    "    +5 for picking object\n",
    "    -1 for slow/jerky moves\n",
    "    -5 for dropping/missing object\n",
    "\n",
    "**Transitions:**\n",
    "    Deterministic - robot knows exactly what happens after each action\n",
    "\n",
    "**Goal:**\n",
    "    Learn to pick and place smoothly and quickly using RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596c8c1",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Problem 2 [20 Marks] - 2x2 Gridworld & Value Iteration\n",
    "\n",
    ">**The Grid:**\n",
    "\n",
    "We have a 2x2 grid with 4 states:\n",
    "\n",
    "`s1  s2`\n",
    "`s3  s4`\n",
    "\n",
    "Actions = up, down, left, right\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "- s1 = +5\n",
    "- s2 = +10\n",
    "- s3 = +1\n",
    "- s4 = +2\n",
    "\n",
    "**Initial policy:** always move UP.\n",
    "\n",
    "---\n",
    "\n",
    ">**Value Iteration - What is it?**\n",
    "\n",
    "Value Iteration helps us figure out the best value of each state.\n",
    "We update the value of each state based on:\n",
    "\n",
    "`V(s) = max over a [ sum of (reward + discounted future value) ]`\n",
    "\n",
    "**Let’s use:**\n",
    "\n",
    "`Discount factor γ = 1 (for simplicity)`\n",
    "\n",
    "----\n",
    "\n",
    "### **Iteration 1:**\n",
    "\n",
    "#### **Step 1: Initial Values**\n",
    "\n",
    "**Let’s say initially:**\n",
    "\n",
    "- `V(s1) = 0`\n",
    "-` V(s2) = 0`\n",
    "- `V(s3) = 0`\n",
    "- `V(s4) = 0`\n",
    "\n",
    "#### **Step 2: Update Each State's Value**\n",
    "\n",
    "We calculate value of each state by looking at the reward from going to the next state.\n",
    "\n",
    "**Let’s assume:**\n",
    "\n",
    "- Valid moves take you to next state\n",
    "- Invalid moves = stay in the same state\n",
    "\n",
    "**Let’s update:**\n",
    "\n",
    "**s1:**\n",
    "\n",
    "Up → stays in s1 → reward = 5 + V(s1) = 5 + 0 = 5\n",
    "\n",
    "**s2:**\n",
    "\n",
    "Up → stays in s2 → reward = 10 + V(s2) = 10 + 0 = 10\n",
    "\n",
    "**s3:**\n",
    "\n",
    "Up → goes to s1 → reward = 5 + V(s1) = 5 + 0 = 5\n",
    "\n",
    "**s4:**\n",
    "\n",
    "Up → goes to s2 → reward = 10 + V(s2) = 10 + 0 = 10\n",
    "\n",
    ">**Updated Values after Iteration 1:**\n",
    "\n",
    "`V(s1) = 5`\n",
    "`V(s2) = 10`\n",
    "`V(s3) = 5`\n",
    "`V(s4) = 10`\n",
    "\n",
    "----\n",
    "\n",
    "### **Iteration 2:**\n",
    "\n",
    "**Now use updated values:**\n",
    "\n",
    "**s1:**\n",
    "\n",
    "Up → s1 → 5 + V(s1) = 5 + 5 = 10\n",
    "\n",
    "**s2:**\n",
    "\n",
    "Up → s2 → 10 + V(s2) = 10 + 10 = 20\n",
    "\n",
    "**s3:**\n",
    "\n",
    "Up → s1 → 5 + V(s1) = 5 + 5 = 10\n",
    "\n",
    "**s4:**\n",
    "\n",
    "Up → s2 → 10 + V(s2) = 10 + 10 = 20\n",
    "\n",
    ">**Updated Values after Iteration 2:**\n",
    "\n",
    "`V(s1) = 10`\n",
    "`V(s2) = 20`\n",
    "`V(s3) = 10`\n",
    "`V(s4) = 20`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb058e",
   "metadata": {},
   "source": [
    "## ** Problem 3 – 5x5 Gridworld using Value Iteration:**\n",
    "\n",
    ">**Gridworld Setup**\n",
    "\n",
    "**We have a 5x5 grid, like this (rows and columns like a matrix):**\n",
    "\n",
    "s0,0  s0,1  s0,2  s0,3  s0,4\n",
    "s1,0  s1,1  s1,2  s1,3  s1,4\n",
    "s2,0  s2,1  s2,2  s2,3  s2,4\n",
    "s3,0  s3,1  s3,2  s3,3  s3,4\n",
    "s4,0  s4,1  s4,2  s4,3  s4,4\n",
    "\n",
    "----\n",
    "\n",
    ">**What is special about it?**\n",
    "\n",
    "- **Goal state** = `s4,4` → Reward = **+10**\n",
    "- **Grey bad states** = `s0,4`, `s2,2`, `s3,0` → Reward = **−5**\n",
    "- **All other states** → Reward = **−1**\n",
    "\n",
    "---\n",
    "\n",
    ">**Actions:**\n",
    "\n",
    "**Each state allows these 4 moves:**\n",
    "\n",
    "- `→ (right)`\n",
    "- `↓ (down)`\n",
    "- `← (left)`\n",
    "- `↑ (up)`\n",
    "\n",
    "If move is invalid (hits wall), the agent stays in the same state.\n",
    "\n",
    "----\n",
    "\n",
    "#### **Task 1: Update MDP code**\n",
    "\n",
    "**We’ll write code that:**\n",
    "\n",
    "- Builds the 5x5 environment\n",
    "- Assigns rewards to each state\n",
    "- Runs Value Iteration\n",
    "\n",
    "**Shows:**\n",
    "\n",
    "- V* (best values)\n",
    "- π* (best policy → direction to move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a362f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Grid size\n",
    "rows, cols = 5, 5\n",
    "\n",
    "# All states\n",
    "states = [(i, j) for i in range(rows) for j in range(cols)]\n",
    "\n",
    "# Goal and bad (grey) states\n",
    "goal_state = (4, 4)\n",
    "grey_states = [(0, 4), (2, 2), (3, 0)]\n",
    "\n",
    "# Rewards\n",
    "def get_reward(state):\n",
    "    if state == goal_state:\n",
    "        return 10\n",
    "    elif state in grey_states:\n",
    "        return -5\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# All possible actions\n",
    "actions = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros((rows, cols))\n",
    "\n",
    "# Value Iteration function\n",
    "def value_iteration(V, threshold=1e-4):\n",
    "    policy = np.full((rows, cols), '', dtype=object)\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = V.copy()\n",
    "\n",
    "        for state in states:\n",
    "            if state == goal_state:\n",
    "                continue  # skip terminal state\n",
    "\n",
    "            i, j = state\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            for action, move in actions.items():\n",
    "                ni, nj = i + move[0], j + move[1]\n",
    "\n",
    "                # Stay in the same state if hitting wall\n",
    "                if 0 <= ni < rows and 0 <= nj < cols:\n",
    "                    next_state = (ni, nj)\n",
    "                else:\n",
    "                    next_state = (i, j)\n",
    "\n",
    "                reward = get_reward(next_state)\n",
    "                value = reward + gamma * V[next_state]\n",
    "\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = action\n",
    "\n",
    "            new_V[i, j] = max_value\n",
    "            policy[i, j] = best_action\n",
    "            delta = max(delta, abs(V[i, j] - new_V[i, j]))\n",
    "\n",
    "        V[:] = new_V\n",
    "        iteration += 1\n",
    "\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    return V, policy, iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65386a",
   "metadata": {},
   "source": [
    ">**What is the code doing?**\n",
    "\n",
    "Sets up all states in a 5x5 grid\n",
    "\n",
    "**Assigns rewards:**\n",
    "\n",
    "- +10 to goal\n",
    "- -5 to bad states\n",
    "- -1 to all others\n",
    "\n",
    "**For each state:**\n",
    "\n",
    "- Tries all 4 directions\n",
    "- Picks the best one\n",
    "- Repeats until values stop changing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd510e28",
   "metadata": {},
   "source": [
    "**Display the Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed7543f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function (V*):\n",
      "[' -0.43', '  0.63', '  1.81', '  3.12', '  4.58']\n",
      "['  0.63', '  1.81', '  3.12', '  4.58', '  6.20']\n",
      "['  1.81', '  3.12', '  4.58', '  6.20', '  8.00']\n",
      "['  3.12', '  4.58', '  6.20', '  8.00', ' 10.00']\n",
      "['  4.58', '  6.20', '  8.00', ' 10.00', '  0.00']\n",
      "\n",
      "Optimal Policy (π*):\n",
      "  ↓    ↓    ↓    ↓    ↓  \n",
      "  ↓    ↓    →    ↓    ↓  \n",
      "  →    ↓    ↓    ↓    ↓  \n",
      "  ↓    ↓    ↓    ↓    ↓  \n",
      "  →    →    →    →    G  \n"
     ]
    }
   ],
   "source": [
    "V_star, policy_star, num_iters = value_iteration(V)\n",
    "\n",
    "# Print value function\n",
    "print(\"Optimal Value Function (V*):\")\n",
    "for row in V_star:\n",
    "    print(['{0: >6.2f}'.format(v) for v in row])\n",
    "\n",
    "# Print optimal policy\n",
    "print(\"\\nOptimal Policy (π*):\")\n",
    "for i in range(rows):\n",
    "    row = ''\n",
    "    for j in range(cols):\n",
    "        if (i, j) == goal_state:\n",
    "            row += '  G  '\n",
    "        else:\n",
    "            act = policy_star[i, j]\n",
    "            symbol = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}[act]\n",
    "            row += f'  {symbol}  '\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e773f22",
   "metadata": {},
   "source": [
    "#### **Task 2: In-Place Value Iteration**\n",
    "\n",
    "- Same idea, but we update values in-place instead of using a copy.This makes the new values affect future updates immediately in the same loop.\n",
    "\n",
    "**In-Place Code Variation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29185111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In-Place Optimal Value Function (V*):\n",
      "[' -0.43', '  0.63', '  1.81', '  3.12', '  4.58']\n",
      "['  0.63', '  1.81', '  3.12', '  4.58', '  6.20']\n",
      "['  1.81', '  3.12', '  4.58', '  6.20', '  8.00']\n",
      "['  3.12', '  4.58', '  6.20', '  8.00', ' 10.00']\n",
      "['  4.58', '  6.20', '  8.00', ' 10.00', '  0.00']\n",
      "\n",
      "In-Place Optimal Policy (π*):\n",
      "  ↓    ↓    ↓    ↓    ↓  \n",
      "  ↓    ↓    →    ↓    ↓  \n",
      "  →    ↓    ↓    ↓    ↓  \n",
      "  ↓    ↓    ↓    ↓    ↓  \n",
      "  →    →    →    →    G  \n"
     ]
    }
   ],
   "source": [
    "# Function\n",
    "def in_place_value_iteration(V, threshold=1e-4):\n",
    "    policy = np.full((rows, cols), '', dtype=object)\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for state in states:\n",
    "            if state == goal_state:\n",
    "                continue\n",
    "\n",
    "            i, j = state\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            for action, move in actions.items():\n",
    "                ni, nj = i + move[0], j + move[1]\n",
    "                if 0 <= ni < rows and 0 <= nj < cols:\n",
    "                    next_state = (ni, nj)\n",
    "                else:\n",
    "                    next_state = (i, j)\n",
    "\n",
    "                reward = get_reward(next_state)\n",
    "                value = reward + gamma * V[next_state]\n",
    "\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = action\n",
    "\n",
    "            delta = max(delta, abs(V[i, j] - max_value))\n",
    "            V[i, j] = max_value\n",
    "            policy[i, j] = best_action\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    return V, policy, iteration\n",
    "V_in_place = np.zeros((rows, cols))\n",
    "V_in_place, policy_in_place, num_iters_in_place = in_place_value_iteration(V_in_place)\n",
    "print(\"\\nIn-Place Optimal Value Function (V*):\")\n",
    "for row in V_in_place:\n",
    "    print(['{0: >6.2f}'.format(v) for v in row])\n",
    "print(\"\\nIn-Place Optimal Policy (π*):\")\n",
    "for i in range(rows):\n",
    "    row = ''\n",
    "    for j in range(cols):\n",
    "        if (i, j) == goal_state:\n",
    "            row += '  G  '\n",
    "        else:\n",
    "            act = policy_in_place[i, j]\n",
    "            symbol = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}[act]\n",
    "            row += f'  {symbol}  '\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388122ab",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "> #### **Comparison Between Classic and In-Place**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d2f03",
   "metadata": {},
   "source": [
    "| Feature         | Classic | In-Place          |\n",
    "| --------------- | ------- | ----------------- |\n",
    "| Uses copy of V? | Yes     | No                |\n",
    "| Speed           | Slower  | Faster (usually)  |\n",
    "| Memory          | More    | Less              |\n",
    "| Accuracy        | Same    | Same (eventually) |\n",
    "| Complexity      | Higher  | Lower             |\n",
    "\n",
    "\n",
    "----\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e5ae",
   "metadata": {},
   "source": [
    "## **Problem 4 – Off-Policy Monte Carlo with Importance Sampling**\n",
    "\n",
    ">**What are we doing here?**\n",
    "\n",
    "We are using Monte Carlo method (learning from episodes), but we’re learning from a different policy than the one we want to learn!\n",
    "\n",
    "That’s called Off-policy learning.\n",
    "\n",
    "\n",
    ">**Goal:**\n",
    "\n",
    "Estimate the value of each state using:\n",
    "\n",
    "A random behavior policy (used to explore)\n",
    "\n",
    "A greedy target policy (what we want to learn)\n",
    "\n",
    "Use importance sampling to correct the difference\n",
    "\n",
    ">**Environment: Same 5x5 Gridworld as Problem 3**\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "- +10 for goal (s4,4)\n",
    "- -5 for grey states (s0,4, s2,2, s3,0)\n",
    "- -1 for all others\n",
    "- Actions: up, down, left, right\n",
    "\n",
    "**γ (discount factor) = 0.9**\n",
    "\n",
    "**Step-by-step Plan**\n",
    "\n",
    "**1. Define Environment**\n",
    "\n",
    "Same as before\n",
    "\n",
    "**2. Behavior Policy (b)**\n",
    "\n",
    "Random — every action has equal chance\n",
    "\n",
    "**3. Target Policy (π)**\n",
    "\n",
    "Greedy — always picks best action based on current estimate\n",
    "\n",
    "**4. Generate Episodes**\n",
    "\n",
    "Use behavior policy to play full episodes (until goal)\n",
    "\n",
    "**5. Estimate Value Function**\n",
    "\n",
    "Use importance sampling to adjust the value based on how different b and π are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900c42e",
   "metadata": {},
   "source": [
    "### **Off-policy MC with Importance Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "685f870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "rows, cols = 5, 5\n",
    "states = [(i, j) for i in range(rows) for j in range(cols)]\n",
    "\n",
    "goal_state = (4, 4)\n",
    "grey_states = [(0, 4), (2, 2), (3, 0)]\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_map = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# Reward function\n",
    "def get_reward(state):\n",
    "    if state == goal_state:\n",
    "        return 10\n",
    "    elif state in grey_states:\n",
    "        return -5\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Behavior policy: Random\n",
    "def behavior_policy():\n",
    "    return random.choice(actions)\n",
    "\n",
    "# Get next state\n",
    "def next_state(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < rows and 0 <= nj < cols:\n",
    "        return (ni, nj)\n",
    "    return state  # invalid move\n",
    "\n",
    "# Generate one episode using behavior policy\n",
    "def generate_episode():\n",
    "    state = random.choice(states[:-1])  # start at any state except goal\n",
    "    episode = []\n",
    "\n",
    "    while state != goal_state:\n",
    "        action = behavior_policy()\n",
    "        next_s = next_state(state, action)\n",
    "        reward = get_reward(next_s)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_s\n",
    "\n",
    "    return episode\n",
    "\n",
    "# Initialize\n",
    "V = {s: 0 for s in states}\n",
    "C = {s: 0 for s in states}\n",
    "pi = {s: random.choice(actions) for s in states}\n",
    "\n",
    "# Off-policy MC with Importance Sampling\n",
    "def off_policy_mc(num_episodes=10000):\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode()\n",
    "        G = 0\n",
    "        W = 1  # Importance Sampling Ratio\n",
    "\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            C[state] += W\n",
    "            V[state] += (W / C[state]) * (G - V[state])\n",
    "\n",
    "            # Update greedy policy\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            for a in actions:\n",
    "                s_prime = next_state(state, a)\n",
    "                r = get_reward(s_prime)\n",
    "                val = r + gamma * V[s_prime]\n",
    "                if val > best_value:\n",
    "                    best_value = val\n",
    "                    best_action = a\n",
    "            pi[state] = best_action\n",
    "\n",
    "            if action != pi[state]:\n",
    "                break  # stop updating if behavior and target policy differ\n",
    "            W *= 1 / 0.25  # each action in behavior policy has prob = 0.25\n",
    "\n",
    "    return V, pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd25947",
   "metadata": {},
   "source": [
    "Run and Show Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "472bde51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State Values (Monte Carlo):\n",
      " -0.43  -0.08   1.81   1.55   1.13 \n",
      "  0.63   0.18   0.90   3.08   3.28 \n",
      " -2.19   1.00   3.33   3.83   5.84 \n",
      "  1.43   1.46   3.64   6.23   7.60 \n",
      "  1.70   4.23   5.82   7.57   0.00 \n",
      "\n",
      "Learned Policy (π*) from Monte Carlo:\n",
      "  ↓    ↓    ↑    ↓    ↓  \n",
      "  →    ↓    →    ↓    ↓  \n",
      "  →    ↓    →    ↓    ↓  \n",
      "  ↓    ↓    →    →    ↓  \n",
      "  →    →    →    →    G   \n"
     ]
    }
   ],
   "source": [
    "V_mc, pi_mc = off_policy_mc()\n",
    "\n",
    "print(\"Estimated State Values (Monte Carlo):\")\n",
    "for i in range(rows):\n",
    "    row = ''\n",
    "    for j in range(cols):\n",
    "        row += f'{V_mc[(i,j)]:6.2f} '\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nLearned Policy (π*) from Monte Carlo:\")\n",
    "for i in range(rows):\n",
    "    row = ''\n",
    "    for j in range(cols):\n",
    "        if (i, j) == goal_state:\n",
    "            row += '  G   '\n",
    "        else:\n",
    "            act = pi_mc[(i, j)]\n",
    "            symbol = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}[act]\n",
    "            row += f'  {symbol}  '\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b811fe",
   "metadata": {},
   "source": [
    "## Explanation of Code\n",
    "\n",
    "We simulate 10,000 episodes using the random behavior policy\n",
    "\n",
    "For each episode:\n",
    "\n",
    "Calculate total reward (G)\n",
    "\n",
    "Use importance sampling to adjust value estimates\n",
    "\n",
    "Update greedy target policy based on best action\n",
    "\n",
    "If behavior and target policy don’t match, stop updating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a2d22",
   "metadata": {},
   "source": [
    "### **Comparision:**\n",
    "\n",
    "This assignment covered four major topics in reinforcement learning:\n",
    "\n",
    "\n",
    "| Problem | Title                                           | Focus                                |\n",
    "| ------- | ----------------------------------------------- | ------------------------------------ |\n",
    "| 1       | Pick-and-Place Robot (MDP Design)               | MDP modeling in robotics             |\n",
    "| 2       | Value Iteration on 2x2 Gridworld                | Manual value update with policy eval |\n",
    "| 3       | Value Iteration on 5x5 Gridworld                | Classic & In-Place Value Iteration   |\n",
    "| 4       | Off-policy Monte Carlo with Importance Sampling | Model-free value estimation          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b375e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "----\n",
    "\n",
    "Each task explored different learning approaches, environments, and techniques.\n",
    "\n",
    "### **Method Comparision**\n",
    "\n",
    "| Feature / Method           | MDP Design (P1) | Value Iteration (P2, P3) | Monte Carlo (P4)      |\n",
    "| -------------------------- | --------------- | ------------------------ | --------------------- |\n",
    "| **Requires Model (P)**     |  Yes           |  Yes                    |  No                  |\n",
    "| **Works with Episodes**    |  No            |  No                     |  Yes                 |\n",
    "| **Policy Improvement**     |  Indirect     |  Yes (greedy)           |  Yes (greedy)        |\n",
    "| **Transitions Known**      |  Yes           |  Yes                    |  No                  |\n",
    "| **Reward Use**             |  Explicit      |  Directly used          |  Through Returns (G) |\n",
    "| **Speed to Converge**      |  Very Fast     |  Fast                   |  Slower              |\n",
    "| **Stability**              |  Stable        |  Very stable            |  May vary           |\n",
    "| **Memory Efficient**       |  N/A           | In-place is  Efficient  | Moderate              |\n",
    "| **Real-World Flexibility** |  Low          |  Limited                |  High                |\n",
    "| **Exploration Needed**     |  Not required  |  Not required           |  Required            |\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "#### **Key Learnings by Problem**\n",
    "\n",
    "**Problem 1 – Pick-and-Place Robot (MDP Design)**\n",
    "- Learned how to model a real-world robotic task as an MDP.\n",
    "- Identified states, actions, rewards, and transitions.\n",
    "- Reward design was critical to encourage smooth and successful behavior.\n",
    "- MDPs are powerful for simulation but require full knowledge of environment dynamics.\n",
    "\n",
    "----\n",
    "\n",
    "**Problem 2 – 2x2 Gridworld (Manual Value Iteration)**\n",
    "\n",
    "- Practiced manual value iteration in a simple grid.\n",
    "- Value function was updated using:\n",
    "   `V(s) = R(s) + γ * V(next state)`\n",
    "- Showed how rewards and value estimates propagate through the state space.\n",
    "- Reinforced understanding of Bellman equation.\n",
    "\n",
    "----\n",
    "\n",
    "**Problem 3 – 5x5 Gridworld (Classic & In-Place)**\n",
    "\n",
    "- Applied Value Iteration in a larger environment.\n",
    "- Compared two methods:\n",
    "\n",
    "   - Classic: Uses two arrays (old and new values)\n",
    "   - In-Place: Uses one array, faster updates\n",
    "\n",
    "- Both methods converged to the same optimal policy and value function.\n",
    "- In-Place was faster and more memory-efficient.\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 4 – Off-Policy Monte Carlo with Importance Sampling**\n",
    "\n",
    "- Learned how to estimate value functions using only episodes, not the model.\n",
    "- Used a random behavior policy and corrected it using importance sampling.\n",
    "- Target policy was greedy (selects best action).\n",
    "- Real-world applicable when the transition model is unknown or dynamic.\n",
    "- Slower than Value Iteration, but more flexible in real environments.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "#### **Final Key Takeaways**\n",
    "\n",
    "- Markov Decision Processes (MDPs) help define decision-making tasks.\n",
    "- Value Iteration is a foundational method in model-based RL.\n",
    "- In-place updates improve performance without changing results.\n",
    "- Monte Carlo methods are model-free and rely on actual experience.\n",
    "- Importance sampling bridges the gap between behavior and target policies.\n",
    "\n",
    "---\n",
    "\n",
    ">**When to Use What?**\n",
    "\n",
    "\n",
    "| Scenario                         | Best Method        |\n",
    "| -------------------------------- | ------------------ |\n",
    "| Full model known                 | Value Iteration  |\n",
    "| No model, but episodes available | Monte Carlo      |\n",
    "| Large state space                | In-Place VI / MC |\n",
    "| Real robot/environment           | Monte Carlo      |\n",
    "| Simulated/grid environment       | Value Iteration  |\n",
    "\n",
    "----\n",
    "\n",
    "### **Final Thoughts**\n",
    "\n",
    "- Understanding when and how to use different reinforcement learning methods is key.\n",
    "- Each method has trade-offs between speed, flexibility, and data needs.\n",
    "- This assignment built strong foundations in both model-based and model-free RL approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
