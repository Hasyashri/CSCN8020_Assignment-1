{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0643da03",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f28df2",
   "metadata": {},
   "source": [
    "## **Problem 1 - Pick-and-Place Robot as an MDP**\n",
    "\n",
    "> **What is the goal?**\n",
    "\n",
    "We want a robot arm to pick up an object and place it somewhere. It should do this in a way that is fast and smooth.\n",
    "\n",
    "To make this happen, we use Reinforcement Learning (RL) ‚Äî and first, we turn this into a Markov Decision Process (MDP).\n",
    "\n",
    "> **MDP Components (What RL needs to learn):**\n",
    "\n",
    "States (S) ‚Äì What the robot knows about the world right now.\n",
    "\n",
    "Actions (A) ‚Äì What the robot can do next.\n",
    "\n",
    "Rewards (R) ‚Äì How good or bad the action was.\n",
    "\n",
    "Transitions (T) ‚Äì What happens when you do an action in a state.\n",
    "\n",
    "Goal ‚Äì Learn a policy (way to act) to maximize rewards over time.\n",
    "\n",
    "### Step 1: Define States (S)\n",
    "\n",
    "The state is the situation the robot is in.\n",
    "\n",
    "State can include:\n",
    "\n",
    "Position of robot arm joints (angles)\n",
    "\n",
    "Velocity of each joint\n",
    "\n",
    "Position of the object to be picked\n",
    "\n",
    "Position of where to place it\n",
    "\n",
    "### Example:\n",
    "\n",
    "S = [joint1_angle, joint1_velocity, joint2_angle, joint2_velocity, object_position, target_position]\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd849f9",
   "metadata": {},
   "source": [
    "### Step 2: Define Actions (A)\n",
    "\n",
    "The actions are what the robot can do.\n",
    "\n",
    "Actions could be:\n",
    "\n",
    "Increase or decrease motor speed\n",
    "\n",
    "Move joint slightly left or right\n",
    "\n",
    "Open or close the gripper\n",
    "\n",
    "**Example:**\n",
    "\n",
    "A = [move_joint1_left, move_joint1_right, move_joint2_left, move_joint2_right, open_gripper, close_gripper]\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Define Rewards (R)\n",
    "\n",
    "Rewards tell the robot if it did good or bad.\n",
    "\n",
    "+10 ‚Üí If the object is successfully placed at the target\n",
    "\n",
    "+5 ‚Üí If the object is picked up\n",
    "\n",
    "-1 ‚Üí If the movement is jerky or slow\n",
    "\n",
    "-5 ‚Üí If the robot drops the object or misses\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Transitions (T)\n",
    "\n",
    "What happens when an action is taken in a state.\n",
    "\n",
    "In this robot, transitions are deterministic (we can predict what happens)\n",
    "\n",
    "For example:\n",
    "\n",
    "If joint1 moves right by 5¬∞, then next angle = angle + 5¬∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f49607",
   "metadata": {},
   "source": [
    "### Final MDP Summary:\n",
    "\n",
    "States (S):\n",
    "    Positions and velocities of robot arm joints, object location, target location\n",
    "\n",
    "Actions (A):\n",
    "    Small motor commands: move joints, open/close gripper\n",
    "\n",
    "Rewards (R):\n",
    "    +10 for placing object correctly\n",
    "    +5 for picking object\n",
    "    -1 for slow/jerky moves\n",
    "    -5 for dropping/missing object\n",
    "\n",
    "Transitions:\n",
    "    Deterministic - robot knows exactly what happens after each action\n",
    "\n",
    "Goal:\n",
    "    Learn to pick and place smoothly and quickly using RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596c8c1",
   "metadata": {},
   "source": [
    "### Problem 2 [20 Marks] - 2x2 Gridworld & Value Iteration\n",
    "\n",
    ">The Grid:\n",
    "\n",
    "We have a 2x2 grid with 4 states:\n",
    "\n",
    "s1  s2\n",
    "s3  s4\n",
    "\n",
    "\n",
    "Actions = up, down, left, right\n",
    "Rewards:\n",
    "\n",
    "s1 = +5\n",
    "\n",
    "s2 = +10\n",
    "\n",
    "s3 = +1\n",
    "\n",
    "s4 = +2\n",
    "\n",
    "Initial policy: always move UP.\n",
    "\n",
    "üîÅ Value Iteration - What is it?\n",
    "\n",
    "Value Iteration helps us figure out the best value of each state.\n",
    "We update the value of each state based on:\n",
    "\n",
    "V(s) = max over a [ sum of (reward + discounted future value) ]\n",
    "\n",
    "\n",
    "Let‚Äôs use:\n",
    "\n",
    "Discount factor Œ≥ = 1 (for simplicity)\n",
    "\n",
    "----\n",
    "\n",
    "### **Iteration 1:**\n",
    "\n",
    "Step 1: Initial Values\n",
    "\n",
    "Let‚Äôs say initially:\n",
    "\n",
    "V(s1) = 0\n",
    "V(s2) = 0\n",
    "V(s3) = 0\n",
    "V(s4) = 0\n",
    "\n",
    "Step 2: Update Each State's Value\n",
    "\n",
    "We calculate value of each state by looking at the reward from going to the next state.\n",
    "\n",
    "Let‚Äôs assume:\n",
    "\n",
    "Valid moves take you to next state\n",
    "\n",
    "Invalid moves = stay in the same state\n",
    "\n",
    "Let‚Äôs update:\n",
    "\n",
    "s1:\n",
    "\n",
    "Up ‚Üí stays in s1 ‚Üí reward = 5 + V(s1) = 5 + 0 = 5\n",
    "\n",
    "s2:\n",
    "\n",
    "Up ‚Üí stays in s2 ‚Üí reward = 10 + V(s2) = 10 + 0 = 10\n",
    "\n",
    "s3:\n",
    "\n",
    "Up ‚Üí goes to s1 ‚Üí reward = 5 + V(s1) = 5 + 0 = 5\n",
    "\n",
    "s4:\n",
    "\n",
    "Up ‚Üí goes to s2 ‚Üí reward = 10 + V(s2) = 10 + 0 = 10\n",
    "\n",
    "‚úÖ Updated Values after Iteration 1:\n",
    "V(s1) = 5\n",
    "V(s2) = 10\n",
    "V(s3) = 5\n",
    "V(s4) = 10\n",
    "\n",
    "üîÅ Iteration 2:\n",
    "\n",
    "Now use updated values:\n",
    "\n",
    "s1:\n",
    "\n",
    "Up ‚Üí s1 ‚Üí 5 + V(s1) = 5 + 5 = 10\n",
    "\n",
    "s2:\n",
    "\n",
    "Up ‚Üí s2 ‚Üí 10 + V(s2) = 10 + 10 = 20\n",
    "\n",
    "s3:\n",
    "\n",
    "Up ‚Üí s1 ‚Üí 5 + V(s1) = 5 + 5 = 10\n",
    "\n",
    "s4:\n",
    "\n",
    "Up ‚Üí s2 ‚Üí 10 + V(s2) = 10 + 10 = 20\n",
    "\n",
    "‚úÖ Updated Values after Iteration 2:\n",
    "V(s1) = 10\n",
    "V(s2) = 20\n",
    "V(s3) = 10\n",
    "V(s4) = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb058e",
   "metadata": {},
   "source": [
    "### Problem 3 ‚Äì 5x5 Gridworld using Value Iteration\n",
    "\n",
    "üó∫Ô∏è Gridworld Setup:\n",
    "\n",
    "We have a 5x5 grid, like this (rows and columns like a matrix):\n",
    "\n",
    "s0,0  s0,1  s0,2  s0,3  s0,4\n",
    "s1,0  s1,1  s1,2  s1,3  s1,4\n",
    "s2,0  s2,1  s2,2  s2,3  s2,4\n",
    "s3,0  s3,1  s3,2  s3,3  s3,4\n",
    "s4,0  s4,1  s4,2  s4,3  s4,4\n",
    "\n",
    "üß† What is special about it?\n",
    "\n",
    "Goal state = s4,4 ‚Üí Reward = +10\n",
    "\n",
    "Grey bad states = s0,4, s2,2, s3,0 ‚Üí Reward = ‚àí5\n",
    "\n",
    "All other states ‚Üí Reward = ‚àí1\n",
    "\n",
    "üéÆ Actions:\n",
    "\n",
    "Each state allows these 4 moves:\n",
    "\n",
    "‚Üí (right)\n",
    "\n",
    "‚Üì (down)\n",
    "\n",
    "‚Üê (left)\n",
    "\n",
    "‚Üë (up)\n",
    "\n",
    "If move is invalid (hits wall), the agent stays in the same state.\n",
    "\n",
    "üèóÔ∏è Task 1: Update MDP code\n",
    "\n",
    "We‚Äôll write code that:\n",
    "\n",
    "Builds the 5x5 environment\n",
    "\n",
    "Assigns rewards to each state\n",
    "\n",
    "Runs Value Iteration\n",
    "\n",
    "Shows:\n",
    "\n",
    "V* (best values)\n",
    "\n",
    "œÄ* (best policy ‚Üí direction to move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a362f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Grid size\n",
    "rows, cols = 5, 5\n",
    "\n",
    "# All states\n",
    "states = [(i, j) for i in range(rows) for j in range(cols)]\n",
    "\n",
    "# Goal and bad (grey) states\n",
    "goal_state = (4, 4)\n",
    "grey_states = [(0, 4), (2, 2), (3, 0)]\n",
    "\n",
    "# Rewards\n",
    "def get_reward(state):\n",
    "    if state == goal_state:\n",
    "        return 10\n",
    "    elif state in grey_states:\n",
    "        return -5\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# All possible actions\n",
    "actions = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros((rows, cols))\n",
    "\n",
    "# Value Iteration function\n",
    "def value_iteration(V, threshold=1e-4):\n",
    "    policy = np.full((rows, cols), '', dtype=object)\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = V.copy()\n",
    "\n",
    "        for state in states:\n",
    "            if state == goal_state:\n",
    "                continue  # skip terminal state\n",
    "\n",
    "            i, j = state\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            for action, move in actions.items():\n",
    "                ni, nj = i + move[0], j + move[1]\n",
    "\n",
    "                # Stay in the same state if hitting wall\n",
    "                if 0 <= ni < rows and 0 <= nj < cols:\n",
    "                    next_state = (ni, nj)\n",
    "                else:\n",
    "                    next_state = (i, j)\n",
    "\n",
    "                reward = get_reward(next_state)\n",
    "                value = reward + gamma * V[next_state]\n",
    "\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = action\n",
    "\n",
    "            new_V[i, j] = max_value\n",
    "            policy[i, j] = best_action\n",
    "            delta = max(delta, abs(V[i, j] - new_V[i, j]))\n",
    "\n",
    "        V[:] = new_V\n",
    "        iteration += 1\n",
    "\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    return V, policy, iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65386a",
   "metadata": {},
   "source": [
    "What is the code doing?\n",
    "\n",
    "Sets up all states in a 5x5 grid\n",
    "\n",
    "Assigns rewards:\n",
    "\n",
    "+10 to goal\n",
    "\n",
    "-5 to bad states\n",
    "\n",
    "-1 to all others\n",
    "\n",
    "For each state:\n",
    "\n",
    "Tries all 4 directions\n",
    "\n",
    "Picks the best one\n",
    "\n",
    "Repeats until values stop changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed7543f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function (V*):\n",
      "[' -0.43', '  0.63', '  1.81', '  3.12', '  4.58']\n",
      "['  0.63', '  1.81', '  3.12', '  4.58', '  6.20']\n",
      "['  1.81', '  3.12', '  4.58', '  6.20', '  8.00']\n",
      "['  3.12', '  4.58', '  6.20', '  8.00', ' 10.00']\n",
      "['  4.58', '  6.20', '  8.00', ' 10.00', '  0.00']\n",
      "\n",
      "Optimal Policy (œÄ*):\n",
      "  ‚Üì    ‚Üì    ‚Üì    ‚Üì    ‚Üì  \n",
      "  ‚Üì    ‚Üì    ‚Üí    ‚Üì    ‚Üì  \n",
      "  ‚Üí    ‚Üì    ‚Üì    ‚Üì    ‚Üì  \n",
      "  ‚Üì    ‚Üì    ‚Üì    ‚Üì    ‚Üì  \n",
      "  ‚Üí    ‚Üí    ‚Üí    ‚Üí    G  \n"
     ]
    }
   ],
   "source": [
    "V_star, policy_star, num_iters = value_iteration(V)\n",
    "\n",
    "# Print value function\n",
    "print(\"Optimal Value Function (V*):\")\n",
    "for row in V_star:\n",
    "    print(['{0: >6.2f}'.format(v) for v in row])\n",
    "\n",
    "# Print optimal policy\n",
    "print(\"\\nOptimal Policy (œÄ*):\")\n",
    "for i in range(rows):\n",
    "    row = ''\n",
    "    for j in range(cols):\n",
    "        if (i, j) == goal_state:\n",
    "            row += '  G  '\n",
    "        else:\n",
    "            act = policy_star[i, j]\n",
    "            symbol = {'up': '‚Üë', 'down': '‚Üì', 'left': '‚Üê', 'right': '‚Üí'}[act]\n",
    "            row += f'  {symbol}  '\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e773f22",
   "metadata": {},
   "source": [
    "Task 2: In-Place Value Iteration\n",
    "\n",
    "Same idea, but we update values in-place instead of using a copy.\n",
    "\n",
    "This makes the new values affect future updates immediately in the same loop.\n",
    "\n",
    "üîÅ In-Place Code Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29185111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In-Place Optimal Value Function (V*):\n",
      "[' -0.43', '  0.63', '  1.81', '  3.12', '  4.58']\n",
      "['  0.63', '  1.81', '  3.12', '  4.58', '  6.20']\n",
      "['  1.81', '  3.12', '  4.58', '  6.20', '  8.00']\n",
      "['  3.12', '  4.58', '  6.20', '  8.00', ' 10.00']\n",
      "['  4.58', '  6.20', '  8.00', ' 10.00', '  0.00']\n",
      "\n",
      "In-Place Optimal Policy (œÄ*):\n",
      "  ‚Üì    ‚Üì    ‚Üì    ‚Üì    ‚Üì  \n",
      "  ‚Üì    ‚Üì    ‚Üí    ‚Üì    ‚Üì  \n",
      "  ‚Üí    ‚Üì    ‚Üì    ‚Üì    ‚Üì  \n",
      "  ‚Üì    ‚Üì    ‚Üì    ‚Üì    ‚Üì  \n",
      "  ‚Üí    ‚Üí    ‚Üí    ‚Üí    G  \n"
     ]
    }
   ],
   "source": [
    "def in_place_value_iteration(V, threshold=1e-4):\n",
    "    policy = np.full((rows, cols), '', dtype=object)\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for state in states:\n",
    "            if state == goal_state:\n",
    "                continue\n",
    "\n",
    "            i, j = state\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            for action, move in actions.items():\n",
    "                ni, nj = i + move[0], j + move[1]\n",
    "                if 0 <= ni < rows and 0 <= nj < cols:\n",
    "                    next_state = (ni, nj)\n",
    "                else:\n",
    "                    next_state = (i, j)\n",
    "\n",
    "                reward = get_reward(next_state)\n",
    "                value = reward + gamma * V[next_state]\n",
    "\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = action\n",
    "\n",
    "            delta = max(delta, abs(V[i, j] - max_value))\n",
    "            V[i, j] = max_value\n",
    "            policy[i, j] = best_action\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    return V, policy, iteration\n",
    "V_in_place = np.zeros((rows, cols))\n",
    "V_in_place, policy_in_place, num_iters_in_place = in_place_value_iteration(V_in_place)\n",
    "print(\"\\nIn-Place Optimal Value Function (V*):\")\n",
    "for row in V_in_place:\n",
    "    print(['{0: >6.2f}'.format(v) for v in row])\n",
    "print(\"\\nIn-Place Optimal Policy (œÄ*):\")\n",
    "for i in range(rows):\n",
    "    row = ''\n",
    "    for j in range(cols):\n",
    "        if (i, j) == goal_state:\n",
    "            row += '  G  '\n",
    "        else:\n",
    "            act = policy_in_place[i, j]\n",
    "            symbol = {'up': '‚Üë', 'down': '‚Üì', 'left': '‚Üê', 'right': '‚Üí'}[act]\n",
    "            row += f'  {symbol}  '\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388122ab",
   "metadata": {},
   "source": [
    "Comparison Between Classic and In-Place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d2f03",
   "metadata": {},
   "source": [
    "| Feature         | Classic | In-Place          |\n",
    "| --------------- | ------- | ----------------- |\n",
    "| Uses copy of V? | Yes     | No                |\n",
    "| Speed           | Slower  | Faster (usually)  |\n",
    "| Memory          | More    | Less              |\n",
    "| Accuracy        | Same    | Same (eventually) |\n",
    "| Complexity      | Higher  | Lower             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e5ae",
   "metadata": {},
   "source": [
    "Problem 4 ‚Äì Off-Policy Monte Carlo with Importance Sampling\n",
    "üß† What are we doing here?\n",
    "\n",
    "We are using Monte Carlo method (learning from episodes), but we‚Äôre learning from a different policy than the one we want to learn!\n",
    "\n",
    "That‚Äôs called Off-policy learning.\n",
    "\n",
    "üéØ Goal:\n",
    "\n",
    "Estimate the value of each state using:\n",
    "\n",
    "A random behavior policy (used to explore)\n",
    "\n",
    "A greedy target policy (what we want to learn)\n",
    "\n",
    "Use importance sampling to correct the difference\n",
    "\n",
    "üì¶ Environment: Same 5x5 Gridworld as Problem 3\n",
    "\n",
    "Rewards:\n",
    "\n",
    "+10 for goal (s4,4)\n",
    "\n",
    "-5 for grey states (s0,4, s2,2, s3,0)\n",
    "\n",
    "-1 for all others\n",
    "\n",
    "Actions: up, down, left, right\n",
    "\n",
    "Œ≥ (discount factor) = 0.9\n",
    "\n",
    "üîç Step-by-step Plan\n",
    "1. Define Environment\n",
    "\n",
    "Same as before\n",
    "\n",
    "2. Behavior Policy (b)\n",
    "\n",
    "Random ‚Äî every action has equal chance\n",
    "\n",
    "3. Target Policy (œÄ)\n",
    "\n",
    "Greedy ‚Äî always picks best action based on current estimate\n",
    "\n",
    "4. Generate Episodes\n",
    "\n",
    "Use behavior policy to play full episodes (until goal)\n",
    "\n",
    "5. Estimate Value Function\n",
    "\n",
    "Use importance sampling to adjust the value based on how different b and œÄ are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900c42e",
   "metadata": {},
   "source": [
    "# Off-policy MC with Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "685f870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "rows, cols = 5, 5\n",
    "states = [(i, j) for i in range(rows) for j in range(cols)]\n",
    "\n",
    "goal_state = (4, 4)\n",
    "grey_states = [(0, 4), (2, 2), (3, 0)]\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_map = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# Reward function\n",
    "def get_reward(state):\n",
    "    if state == goal_state:\n",
    "        return 10\n",
    "    elif state in grey_states:\n",
    "        return -5\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Behavior policy: Random\n",
    "def behavior_policy():\n",
    "    return random.choice(actions)\n",
    "\n",
    "# Get next state\n",
    "def next_state(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < rows and 0 <= nj < cols:\n",
    "        return (ni, nj)\n",
    "    return state  # invalid move\n",
    "\n",
    "# Generate one episode using behavior policy\n",
    "def generate_episode():\n",
    "    state = random.choice(states[:-1])  # start at any state except goal\n",
    "    episode = []\n",
    "\n",
    "    while state != goal_state:\n",
    "        action = behavior_policy()\n",
    "        next_s = next_state(state, action)\n",
    "        reward = get_reward(next_s)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_s\n",
    "\n",
    "    return episode\n",
    "\n",
    "# Initialize\n",
    "V = {s: 0 for s in states}\n",
    "C = {s: 0 for s in states}\n",
    "pi = {s: random.choice(actions) for s in states}\n",
    "\n",
    "# Off-policy MC with Importance Sampling\n",
    "def off_policy_mc(num_episodes=10000):\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode()\n",
    "        G = 0\n",
    "        W = 1  # Importance Sampling Ratio\n",
    "\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            C[state] += W\n",
    "            V[state] += (W / C[state]) * (G - V[state])\n",
    "\n",
    "            # Update greedy policy\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            for a in actions:\n",
    "                s_prime = next_state(state, a)\n",
    "                r = get_reward(s_prime)\n",
    "                val = r + gamma * V[s_prime]\n",
    "                if val > best_value:\n",
    "                    best_value = val\n",
    "                    best_action = a\n",
    "            pi[state] = best_action\n",
    "\n",
    "            if action != pi[state]:\n",
    "                break  # stop updating if behavior and target policy differ\n",
    "            W *= 1 / 0.25  # each action in behavior policy has prob = 0.25\n",
    "\n",
    "    return V, pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd25947",
   "metadata": {},
   "source": [
    "Run and Show Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "472bde51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State Values (Monte Carlo):\n",
      " -0.43  -0.99   1.81   1.11   1.07 \n",
      " -0.91   0.95   0.85   3.02   3.35 \n",
      " -0.15   0.88   3.10   3.71   5.87 \n",
      "  1.13   2.93   3.62   6.29   7.58 \n",
      "  2.19   4.30   5.89   7.60   0.00 \n",
      "\n",
      "Learned Policy (œÄ*) from Monte Carlo:\n",
      "  ‚Üì    ‚Üí    ‚Üë    ‚Üì    ‚Üì  \n",
      "  ‚Üí    ‚Üì    ‚Üí    ‚Üì    ‚Üì  \n",
      "  ‚Üí    ‚Üì    ‚Üí    ‚Üì    ‚Üì  \n",
      "  ‚Üí    ‚Üì    ‚Üí    ‚Üì    ‚Üì  \n",
      "  ‚Üí    ‚Üí    ‚Üí    ‚Üí    G   \n"
     ]
    }
   ],
   "source": [
    "V_mc, pi_mc = off_policy_mc()\n",
    "\n",
    "print(\"Estimated State Values (Monte Carlo):\")\n",
    "for i in range(rows):\n",
    "    row = ''\n",
    "    for j in range(cols):\n",
    "        row += f'{V_mc[(i,j)]:6.2f} '\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nLearned Policy (œÄ*) from Monte Carlo:\")\n",
    "for i in range(rows):\n",
    "    row = ''\n",
    "    for j in range(cols):\n",
    "        if (i, j) == goal_state:\n",
    "            row += '  G   '\n",
    "        else:\n",
    "            act = pi_mc[(i, j)]\n",
    "            symbol = {'up': '‚Üë', 'down': '‚Üì', 'left': '‚Üê', 'right': '‚Üí'}[act]\n",
    "            row += f'  {symbol}  '\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b811fe",
   "metadata": {},
   "source": [
    "## Explanation of Code\n",
    "\n",
    "We simulate 10,000 episodes using the random behavior policy\n",
    "\n",
    "For each episode:\n",
    "\n",
    "Calculate total reward (G)\n",
    "\n",
    "Use importance sampling to adjust value estimates\n",
    "\n",
    "Update greedy target policy based on best action\n",
    "\n",
    "If behavior and target policy don‚Äôt match, stop updating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b375e",
   "metadata": {},
   "source": [
    "Final Comparison\n",
    "\n",
    "| Feature                 | Value Iteration | Monte Carlo    |\n",
    "| ----------------------- | --------------- | -------------- |\n",
    "| Uses model of env?      | ‚úÖ Yes           | ‚ùå No           |\n",
    "| Needs episodes?         | ‚ùå No            | ‚úÖ Yes          |\n",
    "| Fast convergence?       | ‚úÖ Yes           | ‚ùå Slower       |\n",
    "| Needs transition probs? | ‚úÖ Yes           | ‚ùå No           |\n",
    "| Flexible for real data? | ‚ùå Less          | ‚úÖ More         |\n",
    "| Complexity              | Low             | Medium to High |\n",
    "\n",
    "\n",
    "Final Thoughts\n",
    "\n",
    "Value Iteration is powerful and fast when we know the environment.\n",
    "\n",
    "Monte Carlo is flexible and works well even when we don‚Äôt know the environment's details.\n",
    "\n",
    "Both methods help agents learn how to act optimally over time using rewards and experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
